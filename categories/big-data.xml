<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tomás Delvechio (Publicaciones sobre big data)</title><link>http://tomasdelvechio.github.io/</link><description></description><atom:link href="http://tomasdelvechio.github.io/categories/big-data.xml" rel="self" type="application/rss+xml"></atom:link><language>es</language><copyright>Contents © 2025 &lt;a href="mailto:tdelvechio@unlu.edu.ar"&gt;Tomás Delvechio&lt;/a&gt; 
&lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Thu, 27 Mar 2025 01:20:42 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Introducción a Spark</title><link>http://tomasdelvechio.github.io/blog/introduccion-a-spark/</link><dc:creator>Tomás Delvechio</dc:creator><description>&lt;p&gt;Apache Spark es un framework de procesamiento muy de moda en entornos de procesamiento para Big Data, IoT y Machine Learning.&lt;/p&gt;
&lt;h3&gt;Pipeline&lt;/h3&gt;
&lt;p&gt;El pipeline de trabajo en spark es conocido como &lt;strong&gt;DAG&lt;/strong&gt; (Directed Acyclic Graph) y se basa en armar un grafo de trabajo donde las tareas se sucedan unas a otras segun se establezca.&lt;/p&gt;
&lt;p&gt;Ademas, propone que muchas operaciones que en MapReduce involucran operaciones de entrada salida a disco, se hagan en memoria principal, lo que genera un rendimiento mayor comparado con su predecesor.&lt;/p&gt;
&lt;h3&gt;Lenguajes&lt;/h3&gt;
&lt;p&gt;Spark acepta Java, Scala, Python y R, aunque no todos estan soportados de la misma forma y de manera completa.&lt;/p&gt;
&lt;h3&gt;Otras herramientas&lt;/h3&gt;
&lt;p&gt;Spark ofrece ademas una consola interactiva para Scala, Python y R con capacidades &lt;strong&gt;REPL&lt;/strong&gt; (Read, Evaluate, Print y Loop).&lt;/p&gt;
&lt;p&gt;Ademas del core, Spark ofrece algunas librerias para tareas comunes en el ambito de big data: &lt;em&gt;SparkSQL&lt;/em&gt;, &lt;em&gt;Spark Streaming&lt;/em&gt;, &lt;em&gt;Spark MLlib&lt;/em&gt; y &lt;em&gt;Spark GraphX&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Es posible vincular Spark a una Notebook de Jupyter y trabajar desde la interfaz de esta herramienta para hacer pruebas. (TODO: Agregar link a la notebook).&lt;/p&gt;
&lt;h3&gt;Arquitectura&lt;/h3&gt;
&lt;p&gt;Una aplicación Spark consta de un master, que dependiendo de la plataforma puede ser &lt;em&gt;Spark Master&lt;/em&gt; (standalone) &lt;em&gt;Mesos Master&lt;/em&gt; (Apache Mesos) o &lt;em&gt;ResourceManager&lt;/em&gt; (Apache Hadoop - YARN). Luego un proceso driver que se conoce como SparkContext y por ultimo de los workers.&lt;/p&gt;</description><category>big data</category><category>distributed systems</category><category>programming</category><category>spark</category><guid>http://tomasdelvechio.github.io/blog/introduccion-a-spark/</guid><pubDate>Sat, 08 Sep 2018 18:15:18 GMT</pubDate></item></channel></rss>